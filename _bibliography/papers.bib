---
---

@string{aps = {American Physical Society,}}

@article{manishhuang,
      title={Optimizing Code Runtime Performance through Context-Aware Retrieval-Augmented Generation}, 
      author={Manish Acharya and Yifan Zhang* and Kevin Leach and Yu Huang},
      abstract={Optimizing software performance through auto-
                mated code refinement offers a promising avenue for enhancing
                execution speed and efficiency. Despite recent advancements in
                LLMs, a significant gap remains in their ability to perform in-
                depth program analysis. This study introduces AUTOPATCH, an
                in-context learning approach designed to bridge this gap by en-
                abling LLMs to automatically generate optimized code. Inspired
                by how programmers learn and apply knowledge to optimize
                software, AUTOPATCH incorporates three key components: (1)
                an analogy-driven framework to align LLM optimization with
                human cognitive processes, (2) a unified approach that integrates
                historical code examples and CFG analysis for context-aware
                learning, and (3) an automated pipeline for generating opti-
                mized code through in-context prompting. Experimental results
                demonstrate that AUTOPATCH achieves a 7.3% improvement
                in execution efficiency over GPT-4o across common generated
                executable code, highlighting its potential to advance automated
                program runtime optimization.},
      year={2024},
      eprint={2407.21037},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      journal={Accepted at the 33rd IEEE/ACM International Conference on Program
Comprehension (ICPC 2025 ERA)},
      code={https://github.com/manishacharya60/rag-optimization},
      preview={rag_optimization.png}
}

@article{friedman2024applicationlargelanguagemodels,
      title={An Application of Large Language Models to Coding Negotiation Transcripts}, 
      author={Ray Friedman and Jaewoo Cho and Manish Acharya and Jeanne Brett and Xuhui Zhan and Ningyu Han and Sriram Kannan and Yingxiang Ma and Jesse Spencer-Smith and Elisabeth Jäckel and Alfred Zerres and Madison Hooper and Katie Babbit and Wendi Adair and Soroush Aslani and Tayfun Aykaç and Chris Bauman and Rebecca Bennett and Garrett Brady and Peggy Briggs and Cheryl Dowie and Chase Eck and Igmar Geiger and Frank Jacob and Molly Kern and Sujin Lee and Leigh Anne Liu and Wu Liu and Jeffrey Loewenstein and Anne Lytle and Li Ma and Michel Mann and Alexandra Mislin and Tyree Mitchell and Hannah Martensen née Nagler and Amit Nandkeolyar and Mara Olekalns and Elena Paliakova and Jennifer Parlamis and Jason Pierce and Nancy Pierce and Robin Pinkley and Nathalie Prime and Jimena Ramirez-Marin and Kevin Rockmann and William Ross and Zhaleh Semnani-Azad and Juliana Schroeder and Philip Smith and Elena Stimmer and Roderick Swaab and Leigh Thompson and Cathy Tinsley and Ece Tuncel and Laurie Weingart and Robert Wilken and JingJing Yao and Zhi-Xue Zhang},
      year={2024},
      eprint={2407.21037},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.21037}, 
      html={https://arxiv.org/abs/2407.21037},
      pdf={Application_of_LLMs_to_Coding_Negotiation_Transcripts.pdf},
      website={https://www.ainegotiationlab-vanderbilt.com},
      preview={application_llm_negotiation.png}
}


